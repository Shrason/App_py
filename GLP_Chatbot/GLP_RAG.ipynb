{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:25: SyntaxWarning: invalid escape sequence '\\*'\n",
      "<>:139: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:25: SyntaxWarning: invalid escape sequence '\\*'\n",
      "<>:139: SyntaxWarning: invalid escape sequence '\\['\n",
      "C:\\Users\\shrad\\AppData\\Local\\Temp\\ipykernel_5260\\1715848578.py:25: SyntaxWarning: invalid escape sequence '\\*'\n",
      "  \"\\*\",\n",
      "C:\\Users\\shrad\\AppData\\Local\\Temp\\ipykernel_5260\\1715848578.py:139: SyntaxWarning: invalid escape sequence '\\['\n",
      "  remove_citations = lambda text: re.sub(\"\\[\\d{1,3}\\]\", \"\", text)\n"
     ]
    }
   ],
   "source": [
    "# Code taken from the Unstructured library https://github.com/Unstructured-IO/unstructured/blob/main/unstructured/cleaners/core.py\n",
    "\n",
    "import re\n",
    "\n",
    "UNICODE_BULLETS = [\n",
    "    \"\\u0095\",\n",
    "    \"\\u2022\",\n",
    "    \"\\u2023\",\n",
    "    \"\\u2043\",\n",
    "    \"\\u3164\",\n",
    "    \"\\u204C\",\n",
    "    \"\\u204D\",\n",
    "    \"\\u2219\",\n",
    "    \"\\u25CB\",\n",
    "    \"\\u25CF\",\n",
    "    \"\\u25D8\",\n",
    "    \"\\u25E6\",\n",
    "    \"\\u2619\",\n",
    "    \"\\u2765\",\n",
    "    \"\\u2767\",\n",
    "    \"\\u29BE\",\n",
    "    \"\\u29BF\",\n",
    "    \"\\u002D\",\n",
    "    \"\",\n",
    "    \"\\*\", \n",
    "    \"\\x95\",\n",
    "    \"·\",\n",
    "]\n",
    "\n",
    "BULLETS_PATTERN = \"|\".join(UNICODE_BULLETS)\n",
    "\n",
    "UNICODE_BULLETS_RE = re.compile(f\"(?:{BULLETS_PATTERN})(?!{BULLETS_PATTERN})\")\n",
    "\n",
    "PARAGRAPH_PATTERN = r\"\\s*\\n\\s*\"  # noqa: W605 NOTE(harrell)\n",
    "\n",
    "PARAGRAPH_PATTERN_RE = re.compile(\n",
    "    f\"((?:{BULLETS_PATTERN})|{PARAGRAPH_PATTERN})(?!{BULLETS_PATTERN}|$)\",\n",
    ")\n",
    "DOUBLE_PARAGRAPH_PATTERN_RE = re.compile(\"(\" + PARAGRAPH_PATTERN + \"){2}\")\n",
    "\n",
    "E_BULLET_PATTERN = re.compile(r\"^e(?=\\s)\", re.MULTILINE)\n",
    "\n",
    "\n",
    "def clean_non_ascii_chars(text) -> str:\n",
    "    \"\"\"Cleans non-ascii characters from unicode string.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    \\x88This text contains non-ascii characters!\\x88\n",
    "        -> This text contains non-ascii characters!\n",
    "    \"\"\"\n",
    "    en = text.encode(\"ascii\", \"ignore\")\n",
    "    return en.decode()\n",
    "\n",
    "def clean_bullets(text: str) -> str:\n",
    "    \"\"\"Cleans unicode bullets from a section of text.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    ●  This is an excellent point! -> This is an excellent point!\n",
    "    \"\"\"\n",
    "    search = UNICODE_BULLETS_RE.match(text)\n",
    "    if search is None:\n",
    "        return text\n",
    "\n",
    "    cleaned_text = UNICODE_BULLETS_RE.sub(\" \", text, 1)\n",
    "    return cleaned_text.strip()\n",
    "\n",
    "def clean_extra_whitespace(text: str) -> str:\n",
    "    \"\"\"Cleans extra whitespace characters that appear between words.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    ITEM 1.     BUSINESS -> ITEM 1. BUSINESS\n",
    "    \"\"\"\n",
    "    cleaned_text = re.sub(r\"[\\xa0\\n]\", \" \", text)\n",
    "    cleaned_text = re.sub(r\"([ ]{2,})\", \" \", cleaned_text)\n",
    "    return cleaned_text.strip()\n",
    "\n",
    "def group_broken_paragraphs(\n",
    "    text: str,\n",
    "    line_split: re.Pattern[str] = PARAGRAPH_PATTERN_RE,\n",
    "    paragraph_split: re.Pattern[str] = DOUBLE_PARAGRAPH_PATTERN_RE,\n",
    ") -> str:\n",
    "    \"\"\"Groups paragraphs that have line breaks for visual/formatting purposes.\n",
    "    For example:\n",
    "\n",
    "    '''The big red fox\n",
    "    is walking down the lane.\n",
    "\n",
    "    At the end of the lane\n",
    "    the fox met a bear.'''\n",
    "\n",
    "    Gets converted to\n",
    "\n",
    "    '''The big red fox is walking down the lane.\n",
    "    At the end of the land the fox met a bear.'''\n",
    "    \"\"\"\n",
    "    paragraphs = paragraph_split.split(text)\n",
    "    clean_paragraphs = []\n",
    "    for paragraph in paragraphs:\n",
    "        if not paragraph.strip():\n",
    "            continue\n",
    "        para_split = line_split.split(paragraph)\n",
    "        all_lines_short = all(len(line.strip().split(\" \")) < 5 for line in para_split)\n",
    "        if UNICODE_BULLETS_RE.match(paragraph.strip()) or E_BULLET_PATTERN.match(paragraph.strip()):\n",
    "            clean_paragraphs.extend(group_bullet_paragraph(paragraph))\n",
    "        elif all_lines_short:\n",
    "            clean_paragraphs.extend([line for line in para_split if line.strip()])\n",
    "        else:\n",
    "            clean_paragraphs.append(re.sub(PARAGRAPH_PATTERN, \" \", paragraph))\n",
    "\n",
    "    return \"\\n\\n\".join(clean_paragraphs)\n",
    "\n",
    "def merge_hyphenated_words(text):\n",
    "    \"\"\"\n",
    "    Merges incorrectly hyphenated words in a given text.\n",
    "\n",
    "    This function uses a regular expression to identify occurrences where a word has been split by\n",
    "    a hyphen followed by whitespace, such as in 'import- ant'. It merges these split parts into a\n",
    "    single word, effectively correcting the text to appear as 'important'.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The text containing hyphenated words to be merged.\n",
    "\n",
    "    Returns:\n",
    "        str: The corrected text with all hyphenated words merged.\n",
    "\n",
    "    Example:\n",
    "        corrected_text = merge_hyphenated_words(\"The document was import- ant for the meeting.\")\n",
    "        print(corrected_text)  # Output: \"The document was important for the meeting.\"\n",
    "    \"\"\"\n",
    "    # Regular expression to find hyphenated words\n",
    "    pattern = r'(\\w+)-\\s+(\\w+)'\n",
    "    # Replace the found patterns by merging the two groups\n",
    "    corrected_text = re.sub(pattern, r'\\1\\2', text)\n",
    "    return corrected_text\n",
    "\n",
    "remove_citations = lambda text: re.sub(\"\\[\\d{1,3}\\]\", \"\", text)\n",
    "\n",
    "def clean(\n",
    "    text: str,\n",
    "    extra_whitespace: bool = False,\n",
    "    broken_paragraphs: bool = False,\n",
    "    bullets: bool = False,\n",
    "    ascii: bool = False,\n",
    "    lowercase: bool = False,\n",
    "    citations: bool = False,\n",
    "    merge_split_words: bool = False,\n",
    "\n",
    ") -> str:\n",
    "    \"\"\"Cleans text.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    cleaned_text = text.lower() if lowercase else text\n",
    "    cleaned_text = (\n",
    "        clean_non_ascii_chars(cleaned_text) if ascii else cleaned_text\n",
    "    )\n",
    "    cleaned_text = remove_citations(cleaned_text) if citations else cleaned_text\n",
    "    cleaned_text = clean_extra_whitespace(cleaned_text) if extra_whitespace else cleaned_text\n",
    "    cleaned_text = clean_bullets(cleaned_text) if bullets else cleaned_text\n",
    "    cleaned_text = merge_hyphenated_words(cleaned_text) if merge_split_words else cleaned_text\n",
    "    return cleaned_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import getpass\n",
    "import nest_asyncio\n",
    "import fitz\n",
    "from dotenv import load_dotenv \n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "#sys.path.append('../helpers')\n",
    "\n",
    "#from text_cleaning_helpers import clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install frontend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_PATH = \"FAQ.pdf\"\n",
    "\n",
    "LLMSHERPA_API_URL = \"https://readers.llmsherpa.com/api/document/developer/parseDocument?renderFormat=all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install llama_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install llama_index.readers.smart_pdf_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.readers.file import PDFReader\n",
    "from llama_index.readers.smart_pdf_loader import SmartPDFLoader\n",
    "\n",
    "\n",
    "#pdf_reader_docs = PDFReader().load_data(PDF_PATH)\n",
    "#smart_pdf_loader_docs = SmartPDFLoader(llmsherpa_api_url=LLMSHERPA_API_URL).load_data(PDF_PATH)\n",
    "simple_directory_reader_docs = SimpleDirectoryReader(input_files=[PDF_PATH]).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install fitz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file static already exists.\n"
     ]
    }
   ],
   "source": [
    "#mkdir static\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --upgrade PyMuPDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "document = fitz.open(PDF_PATH)\n",
    "\n",
    "def extract_text(document, opt=\"text\"):\n",
    "    '''Extract text from a page and returns a list of strings'''\n",
    "    text = document.get_text(opt, sort=True) \n",
    "    text = text.split(\"\\n\")\n",
    "    return text\n",
    "\n",
    "pages = [extract_text(page) for page in document]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document(file_path, pages):\n",
    "    \"\"\"\n",
    "    Opens a PDF file and optionally selects specific pages to create a document object.\n",
    "\n",
    "    This function utilizes the `fitz` library to open a PDF file located at `file_path`. \n",
    "    If a list of `pages` is provided, the function selects only these pages from the document.\n",
    "    This is useful for focusing on certain parts of a PDF without loading the entire document into memory.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): The path to the PDF file to be opened.\n",
    "        pages (list of int, optional): A list of page numbers to select from the PDF. \n",
    "            If `None`, the entire document is loaded.\n",
    "\n",
    "    \"\"\"\n",
    "    document = fitz.open(file_path)\n",
    "    if pages is not None:\n",
    "        document.select(pages)  # Select specific pages if pages are provided\n",
    "    return document\n",
    "\n",
    "\n",
    "def handle_chapter_headers_footers(strings, flag):\n",
    "    \"\"\"\n",
    "    Modify a list of strings based on a specified flag and join them into a single string.\n",
    "\n",
    "    This function first removes any empty strings from the input list. It then checks if the\n",
    "    remaining list has more than three elements. If so, it modifies the list by removing the\n",
    "    first element, last element, or both, based on the value of the flag. The final list is then\n",
    "    joined into a single string with spaces separating the elements.\n",
    "\n",
    "    Parameters:\n",
    "        strings (list of str): The list of strings to modify.\n",
    "        flag (str): A flag indicating the modification to perform on the list:\n",
    "            - 'remove_first': Remove the first element of the list.\n",
    "            - 'remove_last': Remove the last element of the list.\n",
    "            - 'remove_first_last': Remove both the first and last elements of the list.\n",
    "            - 'remove_first_two': Remove the first two elements of the list.\n",
    "            - Any other value leaves the list unchanged.\n",
    "\n",
    "    Returns:\n",
    "        str: A single string composed of the modified list elements, separated by spaces.\n",
    "    \"\"\"\n",
    "    # Filter out empty strings\n",
    "    filtered_strings = [s for s in strings if s]\n",
    "    \n",
    "    # Check if the filtered list has more than three elements\n",
    "    if len(filtered_strings) > 3:\n",
    "        if flag == 'remove_first':\n",
    "            filtered_strings = filtered_strings[1:]  # Slice off the first element\n",
    "        elif flag == 'remove_last':\n",
    "            filtered_strings = filtered_strings[:-1]  # Slice off the last element\n",
    "        elif flag == 'remove_first_last':\n",
    "            filtered_strings = filtered_strings[1:-1]  # Slice off the first and last elements\n",
    "        elif flag == 'remove_first_two':\n",
    "            filtered_strings = filtered_strings[2:]  # Slice off the first two elements\n",
    "    \n",
    "    # Join all strings with a space and return the result\n",
    "    return ' '.join(filtered_strings).strip()\n",
    "\n",
    "def extract_text(page, file_name, title, author, flag, opt=\"text\"):\n",
    "    \"\"\"\n",
    "    Extracts text from a specified page of a document and returns a dictionary containing\n",
    "    the extracted text and associated metadata.\n",
    "\n",
    "    The function first retrieves text from the given `page` object using the specified `opt` method.\n",
    "    It then processes this text to remove chapter headers, footers, and applies various cleaning\n",
    "    procedures according to the `flag` and other parameters set in the `clean` function.\n",
    "\n",
    "    Parameters:\n",
    "        page (fitz.Page): The page object from which to extract text.\n",
    "        file_name (str): The name of the file from which the page is taken.\n",
    "        title (str): The title of the document.\n",
    "        author (str): The author of the document.\n",
    "        flag (str): A flag used to customize how chapter headers and footers are handled.\n",
    "        opt (str, optional): The method of text extraction to be used by `get_text`.\n",
    "            Defaults to \"text\", but can be changed to other methods supported by the library.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with two keys:\n",
    "            - 'text': A string containing the cleaned and processed text from the page.\n",
    "            - 'metadata': A dictionary containing metadata about the text, including the\n",
    "                          page number, file name, title, and author.\n",
    "    \"\"\"\n",
    "    \n",
    "    text = page.get_text(opt, sort=True)\n",
    "\n",
    "    text = text.split(\"\\n\")\n",
    "\n",
    "    text = handle_chapter_headers_footers(text, flag)\n",
    "\n",
    "    text = clean(\n",
    "        text,\n",
    "        extra_whitespace=True,\n",
    "        broken_paragraphs=True,\n",
    "        bullets=True,\n",
    "        ascii=True,\n",
    "        lowercase=False,\n",
    "        citations=True,\n",
    "        merge_split_words=True,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"text\": text,\n",
    "        \"metadata\": {\n",
    "            \"page_number\": page.number,\n",
    "            \"file_name\": file_name,\n",
    "            \"title\": title,\n",
    "            \"author\": author\n",
    "        }\n",
    "    }\n",
    "\n",
    "def extract_texts_from_pdf(file_path, title, author, pages, flag):\n",
    "    document = get_document(file_path, pages)\n",
    "    file_name = os.path.basename(file_path)\n",
    "    extracted_texts = [extract_text(page, file_path, title, author, flag) for page in document]\n",
    "    return extracted_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:4: SyntaxWarning: invalid escape sequence '\\G'\n",
      "<>:11: SyntaxWarning: invalid escape sequence '\\G'\n",
      "<>:18: SyntaxWarning: invalid escape sequence '\\G'\n",
      "<>:25: SyntaxWarning: invalid escape sequence '\\G'\n",
      "<>:32: SyntaxWarning: invalid escape sequence '\\G'\n",
      "<>:39: SyntaxWarning: invalid escape sequence '\\G'\n",
      "<>:46: SyntaxWarning: invalid escape sequence '\\G'\n",
      "<>:53: SyntaxWarning: invalid escape sequence '\\G'\n",
      "<>:60: SyntaxWarning: invalid escape sequence '\\G'\n",
      "<>:67: SyntaxWarning: invalid escape sequence '\\G'\n",
      "<>:74: SyntaxWarning: invalid escape sequence '\\G'\n",
      "<>:81: SyntaxWarning: invalid escape sequence '\\G'\n",
      "<>:88: SyntaxWarning: invalid escape sequence '\\G'\n",
      "<>:95: SyntaxWarning: invalid escape sequence '\\G'\n",
      "<>:102: SyntaxWarning: invalid escape sequence '\\G'\n",
      "<>:109: SyntaxWarning: invalid escape sequence '\\G'\n",
      "<>:116: SyntaxWarning: invalid escape sequence '\\G'\n",
      "<>:123: SyntaxWarning: invalid escape sequence '\\G'\n",
      "<>:130: SyntaxWarning: invalid escape sequence '\\G'\n",
      "<>:137: SyntaxWarning: invalid escape sequence '\\G'\n",
      "<>:144: SyntaxWarning: invalid escape sequence '\\G'\n",
      "<>:151: SyntaxWarning: invalid escape sequence '\\G'\n",
      "<>:158: SyntaxWarning: invalid escape sequence '\\G'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\G'\n",
      "<>:11: SyntaxWarning: invalid escape sequence '\\G'\n",
      "<>:18: SyntaxWarning: invalid escape sequence '\\G'\n",
      "<>:25: SyntaxWarning: invalid escape sequence '\\G'\n",
      "<>:32: SyntaxWarning: invalid escape sequence '\\G'\n",
      "<>:39: SyntaxWarning: invalid escape sequence '\\G'\n",
      "<>:46: SyntaxWarning: invalid escape sequence '\\G'\n",
      "<>:53: SyntaxWarning: invalid escape sequence '\\G'\n",
      "<>:60: SyntaxWarning: invalid escape sequence '\\G'\n",
      "<>:67: SyntaxWarning: invalid escape sequence '\\G'\n",
      "<>:74: SyntaxWarning: invalid escape sequence '\\G'\n",
      "<>:81: SyntaxWarning: invalid escape sequence '\\G'\n",
      "<>:88: SyntaxWarning: invalid escape sequence '\\G'\n",
      "<>:95: SyntaxWarning: invalid escape sequence '\\G'\n",
      "<>:102: SyntaxWarning: invalid escape sequence '\\G'\n",
      "<>:109: SyntaxWarning: invalid escape sequence '\\G'\n",
      "<>:116: SyntaxWarning: invalid escape sequence '\\G'\n",
      "<>:123: SyntaxWarning: invalid escape sequence '\\G'\n",
      "<>:130: SyntaxWarning: invalid escape sequence '\\G'\n",
      "<>:137: SyntaxWarning: invalid escape sequence '\\G'\n",
      "<>:144: SyntaxWarning: invalid escape sequence '\\G'\n",
      "<>:151: SyntaxWarning: invalid escape sequence '\\G'\n",
      "<>:158: SyntaxWarning: invalid escape sequence '\\G'\n",
      "C:\\Users\\shrad\\AppData\\Local\\Temp\\ipykernel_22580\\2996850772.py:4: SyntaxWarning: invalid escape sequence '\\G'\n",
      "  \"file_path\": \"GLP Documents\\GLP_doc_2.pdf\",\n",
      "C:\\Users\\shrad\\AppData\\Local\\Temp\\ipykernel_22580\\2996850772.py:11: SyntaxWarning: invalid escape sequence '\\G'\n",
      "  \"file_path\": \"GLP Documents\\GLP_doc_3.pdf\",\n",
      "C:\\Users\\shrad\\AppData\\Local\\Temp\\ipykernel_22580\\2996850772.py:18: SyntaxWarning: invalid escape sequence '\\G'\n",
      "  \"file_path\": \"GLP Documents\\GLP_doc_4.pdf\",\n",
      "C:\\Users\\shrad\\AppData\\Local\\Temp\\ipykernel_22580\\2996850772.py:25: SyntaxWarning: invalid escape sequence '\\G'\n",
      "  \"file_path\": \"GLP Documents\\GLP_doc_5.pdf\",\n",
      "C:\\Users\\shrad\\AppData\\Local\\Temp\\ipykernel_22580\\2996850772.py:32: SyntaxWarning: invalid escape sequence '\\G'\n",
      "  \"file_path\": \"GLP Documents\\GLP_doc_6.pdf\",\n",
      "C:\\Users\\shrad\\AppData\\Local\\Temp\\ipykernel_22580\\2996850772.py:39: SyntaxWarning: invalid escape sequence '\\G'\n",
      "  \"file_path\": \"GLP Documents\\GLP_doc_7.pdf\",\n",
      "C:\\Users\\shrad\\AppData\\Local\\Temp\\ipykernel_22580\\2996850772.py:46: SyntaxWarning: invalid escape sequence '\\G'\n",
      "  \"file_path\": \"GLP Documents\\GLP_doc_8.pdf\",\n",
      "C:\\Users\\shrad\\AppData\\Local\\Temp\\ipykernel_22580\\2996850772.py:53: SyntaxWarning: invalid escape sequence '\\G'\n",
      "  \"file_path\": \"GLP Documents\\GLP_doc_9.pdf\",\n",
      "C:\\Users\\shrad\\AppData\\Local\\Temp\\ipykernel_22580\\2996850772.py:60: SyntaxWarning: invalid escape sequence '\\G'\n",
      "  \"file_path\": \"GLP Documents\\GLP_doc_10.pdf\",\n",
      "C:\\Users\\shrad\\AppData\\Local\\Temp\\ipykernel_22580\\2996850772.py:67: SyntaxWarning: invalid escape sequence '\\G'\n",
      "  \"file_path\": \"GLP Documents\\GLP_doc_11.pdf\",\n",
      "C:\\Users\\shrad\\AppData\\Local\\Temp\\ipykernel_22580\\2996850772.py:74: SyntaxWarning: invalid escape sequence '\\G'\n",
      "  \"file_path\": \"GLP Documents\\GLP_doc_12.pdf\",\n",
      "C:\\Users\\shrad\\AppData\\Local\\Temp\\ipykernel_22580\\2996850772.py:81: SyntaxWarning: invalid escape sequence '\\G'\n",
      "  \"file_path\": \"GLP Documents\\GLP_doc_13.pdf\",\n",
      "C:\\Users\\shrad\\AppData\\Local\\Temp\\ipykernel_22580\\2996850772.py:88: SyntaxWarning: invalid escape sequence '\\G'\n",
      "  \"file_path\": \"GLP Documents\\GLP_doc_14.pdf\",\n",
      "C:\\Users\\shrad\\AppData\\Local\\Temp\\ipykernel_22580\\2996850772.py:95: SyntaxWarning: invalid escape sequence '\\G'\n",
      "  \"file_path\": \"GLP Documents\\GLP_doc_15.pdf\",\n",
      "C:\\Users\\shrad\\AppData\\Local\\Temp\\ipykernel_22580\\2996850772.py:102: SyntaxWarning: invalid escape sequence '\\G'\n",
      "  \"file_path\": \"GLP Documents\\GLP_doc_16.pdf\",\n",
      "C:\\Users\\shrad\\AppData\\Local\\Temp\\ipykernel_22580\\2996850772.py:109: SyntaxWarning: invalid escape sequence '\\G'\n",
      "  \"file_path\": \"GLP Documents\\GLP_doc_17.pdf\",\n",
      "C:\\Users\\shrad\\AppData\\Local\\Temp\\ipykernel_22580\\2996850772.py:116: SyntaxWarning: invalid escape sequence '\\G'\n",
      "  \"file_path\": \"GLP Documents\\GLP_doc_18.pdf\",\n",
      "C:\\Users\\shrad\\AppData\\Local\\Temp\\ipykernel_22580\\2996850772.py:123: SyntaxWarning: invalid escape sequence '\\G'\n",
      "  \"file_path\": \"GLP Documents\\GLP_doc_19.pdf\",\n",
      "C:\\Users\\shrad\\AppData\\Local\\Temp\\ipykernel_22580\\2996850772.py:130: SyntaxWarning: invalid escape sequence '\\G'\n",
      "  \"file_path\": \"GLP Documents\\GLP_doc_20.pdf\",\n",
      "C:\\Users\\shrad\\AppData\\Local\\Temp\\ipykernel_22580\\2996850772.py:137: SyntaxWarning: invalid escape sequence '\\G'\n",
      "  \"file_path\": \"GLP Documents\\GLP_doc_21.pdf\",\n",
      "C:\\Users\\shrad\\AppData\\Local\\Temp\\ipykernel_22580\\2996850772.py:144: SyntaxWarning: invalid escape sequence '\\G'\n",
      "  \"file_path\": \"GLP Documents\\GLP_doc_22.pdf\",\n",
      "C:\\Users\\shrad\\AppData\\Local\\Temp\\ipykernel_22580\\2996850772.py:151: SyntaxWarning: invalid escape sequence '\\G'\n",
      "  \"file_path\": \"GLP Documents\\GLP_doc_23.pdf\",\n",
      "C:\\Users\\shrad\\AppData\\Local\\Temp\\ipykernel_22580\\2996850772.py:158: SyntaxWarning: invalid escape sequence '\\G'\n",
      "  \"file_path\": \"GLP Documents\\GLP_doc_24.pdf\",\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting texts from GLP_doc_2 by NGCMA...\n",
      "Finished extracting texts from GLP_doc_2.\n",
      "Extracting texts from GLP_doc_3 by NGCMA...\n",
      "Finished extracting texts from GLP_doc_3.\n",
      "Extracting texts from GLP_doc_4 by NGCMA...\n",
      "Finished extracting texts from GLP_doc_4.\n",
      "Extracting texts from GLP_doc_5 by NGCMA...\n",
      "Finished extracting texts from GLP_doc_5.\n",
      "Extracting texts from GLP_doc_6 by NGCMA...\n",
      "Finished extracting texts from GLP_doc_6.\n",
      "Extracting texts from GLP_doc_7 by NGCMA...\n",
      "Finished extracting texts from GLP_doc_7.\n",
      "Extracting texts from GLP_doc_8 by NGCMA...\n",
      "Finished extracting texts from GLP_doc_8.\n",
      "Extracting texts from GLP_doc_9 by NGCMA...\n",
      "Finished extracting texts from GLP_doc_9.\n",
      "Extracting texts from GLP_doc_10 by NGCMA...\n",
      "Finished extracting texts from GLP_doc_10.\n",
      "Extracting texts from GLP_doc_11 by NGCMA...\n",
      "Finished extracting texts from GLP_doc_11.\n",
      "Extracting texts from GLP_doc_12 by NGCMA...\n",
      "Finished extracting texts from GLP_doc_12.\n",
      "Extracting texts from GLP_doc_13 by NGCMA...\n",
      "Finished extracting texts from GLP_doc_13.\n",
      "Extracting texts from GLP_doc_14 by NGCMA...\n",
      "Finished extracting texts from GLP_doc_14.\n",
      "Extracting texts from GLP_doc_15 by NGCMA...\n",
      "Finished extracting texts from GLP_doc_15.\n",
      "Extracting texts from GLP_doc_16 by NGCMA...\n",
      "Finished extracting texts from GLP_doc_16.\n",
      "Extracting texts from GLP_doc_17 by NGCMA...\n",
      "Finished extracting texts from GLP_doc_17.\n",
      "Extracting texts from GLP_doc_18 by NGCMA...\n",
      "Finished extracting texts from GLP_doc_18.\n",
      "Extracting texts from GLP_doc_19 by NGCMA...\n",
      "Finished extracting texts from GLP_doc_19.\n",
      "Extracting texts from GLP_doc_20 by NGCMA...\n",
      "Finished extracting texts from GLP_doc_20.\n",
      "Extracting texts from GLP_doc_21 by NGCMA...\n",
      "Finished extracting texts from GLP_doc_21.\n",
      "Extracting texts from GLP_doc_22 by NGCMA...\n",
      "Finished extracting texts from GLP_doc_22.\n",
      "Extracting texts from GLP_doc_23 by NGCMA...\n",
      "Finished extracting texts from GLP_doc_23.\n",
      "Extracting texts from GLP_doc_24 by NGCMA...\n",
      "Finished extracting texts from GLP_doc_24.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "pdf_files = [\n",
    "    {\n",
    "        \"file_path\": \"FAQ.pdf\", \n",
    "        \"title\": \"FAQ.pdf\", \n",
    "        \"author\": \"OECD\", \n",
    "        \"pages\": list(range(1,10)),\n",
    "        \"flag\": \"remove_last\"\n",
    "        },\n",
    "    \n",
    "    ]\n",
    "    \n",
    "all_texts = []\n",
    "\n",
    "for pdf in pdf_files:\n",
    "    print(f\"Extracting texts from {pdf['title']} by {pdf['author']}...\")\n",
    "    texts = extract_texts_from_pdf(pdf[\"file_path\"], pdf[\"title\"], pdf[\"author\"], pdf[\"pages\"], pdf[\"flag\"])\n",
    "    print(f\"Finished extracting texts from {pdf['title']}.\")\n",
    "    all_texts.extend(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "\n",
    "llama_index_docs = [Document(text=doc[\"text\"], metadata=doc[\"metadata\"]) for doc in all_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.storage.docstore import SimpleDocumentStore\n",
    "from llama_index.core.storage import StorageContext\n",
    "\n",
    "# Create a SimpleDocumentStore and add the documents\n",
    "docstore = SimpleDocumentStore()\n",
    "docstore.add_documents(llama_index_docs)\n",
    "\n",
    "# Create a storage context\n",
    "storage_context = StorageContext.from_defaults(docstore=docstore)\n",
    "\n",
    "# Persist the document store to disk\n",
    "storage_context.persist(\"data/words-of-the-sequence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install llama-index==0.10.37 cohere==5.5.0 openai==1.30.1 llama-index-embeddings-openai==0.1.9 qdrant-client==1.9.1 llama-index-vector-stores-qdrant==0.2.8 llama-index-llms-cohere==0.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install datasets\n",
    "#!pip install llama_index.embeddings.fastembed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install llama-index\n",
    "#!pip install cohere\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install llama_index.embeddings.cohere \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install llama_index.llms.mistralai "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --upgrade llama-index openai pydantic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --upgrade llama-index cohere pydantic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from getpass import getpass\n",
    "import nest_asyncio\n",
    "import llama_index.embeddings.cohere \n",
    "\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "#sys.path.append('../helpers')\n",
    "\n",
    "from llama_def import setup_llm, setup_embed_model, setup_vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "CO_API_KEY = os.environ['CO_API_KEY'] or getpass(\"Enter your Cohere API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.environ['OPENAI_API_KEY'] or getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "QDRANT_URL = os.environ['QDRANT_URL'] or getpass(\"Enter your Qdrant URL:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "QDRANT_API_KEY = os.environ['QDRANT_API_KEY'] or  getpass(\"Enter your Qdrant API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both client and aclient are provided. If using `:memory:` mode, the data between clients is not synced.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.settings import Settings\n",
    "from llama_def import setup_llm, setup_embed_model, setup_vector_store\n",
    "\n",
    "COLLECTION_NAME = \"words-of-the-sequence\"\n",
    "\n",
    "setup_llm(\n",
    "    provider=\"cohere\", \n",
    "    model=\"command-r-plus\", \n",
    "    api_key=CO_API_KEY\n",
    "    )\n",
    "\n",
    "setup_embed_model(\n",
    "    provider=\"openai\", \n",
    "    model_name=\"text-embedding-3-large\",\n",
    "    api_key=OPENAI_API_KEY\n",
    "    )\n",
    "\n",
    "vector_store = setup_vector_store(QDRANT_URL, QDRANT_API_KEY, COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_def import get_documents_from_docstore\n",
    "\n",
    "documents = get_documents_from_docstore(\"data/words-of-the-sequence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_def import get_documents_from_docstore\n",
    "\n",
    "documents = get_documents_from_docstore(\"data/words-of-the-sequence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the chunk size: 1024\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.constants import DEFAULT_CHUNK_SIZE\n",
    "from llama_index.core.node_parser.text import SentenceSplitter\n",
    "from llama_index.core import StorageContext\n",
    "from llama_def import ingest\n",
    "\n",
    "print(f\"This is the chunk size: {DEFAULT_CHUNK_SIZE}\")\n",
    "\n",
    "tranforms = [\n",
    "    SentenceSplitter(chunk_size=DEFAULT_CHUNK_SIZE), \n",
    "    Settings.embed_model\n",
    "    ]\n",
    "\n",
    "nodes = ingest(\n",
    "    documents=documents,\n",
    "    transformations=tranforms,\n",
    "    vector_store=vector_store,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_def import create_index, create_query_engine\n",
    "\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    vector_store=vector_store\n",
    "    )\n",
    "\n",
    "index = create_index(\n",
    "    from_where=\"vector_store\",\n",
    "    embed_model=Settings.embed_model, \n",
    "    vector_store=vector_store, \n",
    "    # storage_context=storage_context\n",
    "    )\n",
    "\n",
    "query_engine = create_query_engine(\n",
    "    index=index, \n",
    "    mode=\"query\",\n",
    "    # llm=Settings.llm\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_def import create_query_pipeline\n",
    "\n",
    "from llama_index.core.query_pipeline import InputComponent\n",
    "\n",
    "input_component = InputComponent()\n",
    "\n",
    "chain = [input_component, query_engine]\n",
    "\n",
    "query_pipeline = create_query_pipeline(chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;2;155;135;227m> Running module 34176422-4338-4854-aa3a-f0e474a7100e with input: \n",
      "input: How should the frequency of QA audits be determined?\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module 9a3256be-d287-405d-b4a8-b89ef904339c with input: \n",
      "input: How should the frequency of QA audits be determined?\n",
      "\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "response = query_pipeline.run(input='How should the frequency of QA audits be determined?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The frequency of QA audits should be determined by the type of inspection being carried out and the associated risks. A risk-based approach allows QA personnel to determine the type of inspection, when to carry it out, and how to allocate resources effectively.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
